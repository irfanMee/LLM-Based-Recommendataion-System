{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ['LD_LIBRARY_PATH'] = 'YOUR_CONDA_ENV/lib'\n",
    "import sys\n",
    "from typing import List\n",
    "\n",
    "import numpy as np \n",
    "import fire\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import EarlyStoppingCallback\n",
    " \n",
    "from peft import (  # noqa: E402\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_int8_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer  # noqa: F402"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = \"baffo32/decapoda-research-llama-7B-hf\"\n",
    " \n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={'':torch.cuda.current_device()},\n",
    ")\n",
    " \n",
    "tokenizer = LlamaTokenizer.from_pretrained(BASE_MODEL)\n",
    " \n",
    "tokenizer.pad_token_id = (0)  # unk. we want this to be different from the eos token\n",
    "\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "    # sorry about the formatting disaster gotta move fast\n",
    "    if data_point[\"input\"]:\n",
    "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. \n",
    "\n",
    "### Instruction:\n",
    "{data_point[\"instruction\"]}\n",
    "\n",
    "### Input:\n",
    "{data_point[\"input\"]}\n",
    "\n",
    "### Response:\n",
    "{data_point[\"output\"]}\"\"\"\n",
    "    else:\n",
    "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.  \n",
    "\n",
    "### Instruction:\n",
    "{data_point[\"instruction\"]}\n",
    "\n",
    "### Response:\n",
    "{data_point[\"output\"]}\"\"\"\n",
    " \n",
    "cutoff_len = 512\n",
    "\n",
    "def tokenize(prompt, add_eos_token=True):\n",
    "        # there's probably a way to do this with the tokenizer settings\n",
    "        # but again, gotta move fast\n",
    "        result = tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=cutoff_len,\n",
    "            padding=False,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        if (\n",
    "            result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "            and len(result[\"input_ids\"]) < cutoff_len\n",
    "            and add_eos_token\n",
    "        ):\n",
    "            result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "            result[\"attention_mask\"].append(1)\n",
    "\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "        return result\n",
    " \n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "        full_prompt = generate_prompt(data_point)\n",
    "        tokenized_full_prompt = tokenize(full_prompt)\n",
    "        if not train_on_inputs:\n",
    "            user_prompt = generate_prompt({**data_point, \"output\": \"\"})\n",
    "            tokenized_user_prompt = tokenize(user_prompt, add_eos_token=False)\n",
    "            user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n",
    "\n",
    "            tokenized_full_prompt[\"labels\"] = [\n",
    "                -100\n",
    "            ] * user_prompt_len + tokenized_full_prompt[\"labels\"][\n",
    "                user_prompt_len:\n",
    "            ]  # could be sped up, probably\n",
    "        return tokenized_full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_r = 8\n",
    "lora_alpha = 16\n",
    "lora_target_modules = [\"q_proj\", \"v_proj\",]\n",
    "lora_dropout = 0.05\n",
    "model = prepare_model_for_int8_training(model)\n",
    "\n",
    "config = LoraConfig(\n",
    "        r=lora_r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        target_modules=lora_target_modules,\n",
    "        lora_dropout=lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_list = []\n",
    "val_data_list = []\n",
    "\n",
    "train_data_path = [\"data/movielens_1m/train.json\"]\n",
    "val_data_path = [\"data/movielens_1m/valid_5000.json\"]\n",
    "for path in train_data_path:\n",
    "    if path.endswith(\".json\"):\n",
    "        train_data_list.append(load_dataset(\"json\", data_files=path))\n",
    "    else:\n",
    "        train_data_list.append(load_dataset(path))\n",
    "\n",
    "for path in val_data_path:\n",
    "        if path.endswith(\".json\"):\n",
    "            val_data_list.append(load_dataset(\"json\", data_files=path))\n",
    "        else:\n",
    "            val_data_list.append(load_dataset(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 1024\n",
    "seed = 0\n",
    "train_on_inputs = True\n",
    "for i in range(len(train_data_list)):\n",
    "        train_data_list[i][\"train\"] = train_data_list[i][\"train\"].shuffle(seed=seed).select(range(sample)) if sample > -1 else train_data_list[i][\"train\"].shuffle(seed=seed)\n",
    "        train_data_list[i][\"train\"] = train_data_list[i][\"train\"].shuffle(seed=seed)\n",
    "        train_data_list[i] = train_data_list[i].map(lambda x: generate_and_tokenize_prompt(x))\n",
    "for i in range(len(val_data_list)):\n",
    "        val_data_list[i] = val_data_list[i].map(lambda x: generate_and_tokenize_prompt(x))\n",
    "train_data = concatenate_datasets([_[\"train\"] for _ in train_data_list])\n",
    "val_data = concatenate_datasets([_[\"train\"] for _ in val_data_list])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data[0]['input_ids']), len(train_data[0]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.print_trainable_parameters()  # Be more transparent about the % of trainable params.\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"data/output/movie/${seed}_${sample}\"\n",
    "micro_batch_size = 4\n",
    "batch_size = 128\n",
    "gradient_accumulation_steps = batch_size // micro_batch_size\n",
    "num_epochs = 5\n",
    "learning_rate = 1e-4\n",
    "group_by_length = False\n",
    "resume_from_checkpoint = None\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=micro_batch_size,\n",
    "        per_device_eval_batch_size=micro_batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        warmup_steps=20,\n",
    "        num_train_epochs=num_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        fp16=True,\n",
    "        logging_steps=8,\n",
    "        optim=\"adamw_torch\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        eval_steps=50,\n",
    "        save_strategy=\"epoch\",\n",
    "        output_dir=output_dir,\n",
    "        save_total_limit=1,\n",
    "        load_best_model_at_end=True,\n",
    "        ddp_find_unused_parameters=None,\n",
    "        group_by_length=group_by_length,\n",
    "        report_to=None,\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForSeq2Seq(\n",
    "        tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    "    ),\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=5)]\n",
    ")\n",
    "model.config.use_cache = False\n",
    "old_state_dict = model.state_dict\n",
    "model.state_dict = (\n",
    "    lambda self, *_, **__: get_peft_model_state_dict(\n",
    "        self, old_state_dict()\n",
    "    )\n",
    ").__get__(model, type(model))\n",
    "\n",
    "if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "    model = torch.compile(model)\n",
    "\n",
    "trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "model.save_pretrained(output_dir)\n",
    "print(\n",
    "        \"\\n If there's a warning about missing keys above, please disregard :)\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
